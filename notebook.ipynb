{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Programming-Language-Trend-and-Sentiment-Analysis\">Programming Language Trend and Sentiment Analysis<a class=\"anchor-link\" href=\"#Programming-Language-Trend-and-Sentiment-Analysis\">¶</a></h1><p><strong>Project by:</strong></p>\n",
    "<p><strong>Satyajeet Maharana,\n",
    "Hitesh Patel,\n",
    "Mike Akiki,\n",
    "Shreya Pandey</strong></p>\n",
    "<p><strong>Team 8</strong></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We plan to analyze stackoverflow data to figure out</p>\n",
    "<p>Trends in programming languages. \n",
    "Analysis of comment will help understanding overall satisfaction of different programmers. \n",
    "To find language in use according to region.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Data-Collection-from-stackoverflow-archive\">Data Collection from stackoverflow archive<a class=\"anchor-link\" href=\"#Data-Collection-from-stackoverflow-archive\">¶</a></h1><p>We collected below data of different sizes from stack overflow archive link : <a href=\"https://archive.org/download/stackexchange\">https://archive.org/download/stackexchange</a></p>\n",
    "<ol>\n",
    "<li>stackoverflow.votes.7z</li>\n",
    "<li>stackoverflow-tags.7z</li>\n",
    "<li>stackoverflow-users.7z</li>\n",
    "<li>stackoverflow-comments.7z</li>\n",
    "<li>stackoverflow-posts.7z</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"ER-Diagram\">ER Diagram<a class=\"anchor-link\" href=\"#ER-Diagram\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Click here for the PDF file of the ER Diagram: <a href=\"https://drive.google.com/file/d/1FmlUIAijHdBy09rG-wf6zZ5WaP5u5hTm/view?usp=sharing\">ER Diagram PDF</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><img alt=\"alt text\" src=\"https://i.ibb.co/WGncwHr/ER-Diagram.jpg\"/></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Data-Cleaning-(converting-xml-file-into-csv)-using-python\">Data Cleaning (converting xml file into csv) using python<a class=\"anchor-link\" href=\"#Data-Cleaning-(converting-xml-file-into-csv)-using-python\">¶</a></h1><p>The datafiles we received were in sizes of about 10GB - 60 GB in XML format. After we ran the below code to ignore the columns we do not need,further clean the data, and  convert the file format to CSV.</p>\n",
    "<h2 id=\"The-below-code-was-run-on-the-Bigdata-server-with-files-present-in-the-Bigdata-local-folders.\">The below code was run on the Bigdata server with files present in the Bigdata local folders.<a class=\"anchor-link\" href=\"#The-below-code-was-run-on-the-Bigdata-server-with-files-present-in-the-Bigdata-local-folders.\">¶</a></h2><p>After Cleaning our data size in csv was within range 100 MBs to 5GB</p>\n",
    "<p>Below is the code:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! /usr/bin/python3\n",
    "\n",
    "import xml.etree.ElementTree as etree\n",
    "import csv\n",
    " \n",
    "with open('stackoverflow_comments.csv', 'w+') as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['Id','PostId','Score','Text','CreationDate','UserId'])\n",
    "    i = 0\n",
    "    for event, elem in etree.iterparse('/bigtemp/DealingF18GB/DealingF18GB8/Project Data/Comments.xml', events=('start','start-ns')):\n",
    "        print(i)\n",
    "        if(elem.attrib.get('Id')!= None):\n",
    "            Id = elem.attrib.get('Id')\n",
    "            if(Id != None):\n",
    "                Id = Id.encode('utf-8')\n",
    "            PostId = elem.attrib.get('PostId')\n",
    "            if(PostId != None):\n",
    "                PostId = PostId.encode('utf-8')\n",
    "            Score = elem.attrib.get('Score')\n",
    "            if(Score != None):\n",
    "              Score = Score.encode('utf-8')\n",
    "            Text = elem.attrib.get('Text')\n",
    "            if(Text != None):\n",
    "                Text = Text.encode('utf-8')\n",
    "            CreationDate = elem.attrib.get('CreationDate')\n",
    "            if(CreationDate!=None):\n",
    "                CreationDate = CreationDate.split('T')[0].encode('utf-8')\n",
    "            UserId = elem.attrib.get('UserId')\n",
    "            if(UserId != None):\n",
    "                UserId = UserId.encode('utf-8')\n",
    "            \n",
    "            filewriter.writerow([Id,PostId,Score,Text,CreationDate,UserId])\n",
    "            elem.clear()\n",
    "            i = i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><strong>Note :</strong> The above code was run in Bigdata server and is not expected to run successfully in Notebooks.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Data-Load-in-Hive\">Data Load in Hive<a class=\"anchor-link\" href=\"#Data-Load-in-Hive\">¶</a></h1><p>We uploaded data into hive and executed Hive Queries to aggregate and get count within valid range with respect to different parameters like region,creationdate,tags,etc</p>\n",
    "<h2 id=\"The-below-code-was-was-run-in-Bigdata-server-directly.-The-required-files-were-uploaded-to-Bigdata-server-using-SFTP-(Cyberduck-software)\">The below code was was run in Bigdata server directly. The required files were uploaded to Bigdata server using SFTP (Cyberduck software)<a class=\"anchor-link\" href=\"#The-below-code-was-was-run-in-Bigdata-server-directly.-The-required-files-were-uploaded-to-Bigdata-server-using-SFTP-(Cyberduck-software)\">¶</a></h2><p>Below are the sample hive Queries:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create external table Stackoverflow_posts (id  int, tags string, owneruserid int, lastedate string, answercount int, creationdate string, acceptedanswerid int)\n",
    "row format delimited fields terminated by ',' \n",
    "location '/user/DealingF18GB8/Project/Hivetables/â€™;\n",
    "tblproperties (\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "create external table stackoverflow_posts  (id int, tags string, owneruserid int, lastdate string, answercount int, creationdate string,acceptedanserid int) row format delimited fields terminated by ','\n",
    "location '/user/DealingF18GB8/Project/Hivetables_stackposts/' ;\n",
    "create external table stackoverflow_comments  (id int, postid int, score int, text string, creationdate string,userid int) row format delimited fields terminated by ','\n",
    "location '/user/DealingF18GB8/Project/Hivetables_stackcomments/' ;\n",
    "create external table stackoverflow_users  (Id int,Reputation int,CreationDate string,LastAccessDate string,Location string,Views int,UpVotes int,DownVotes int,AccountId int) row format delimited fields terminated by ','\n",
    "location '/user/DealingF18GB8/Project/Hivetables_stackusers' ;\n",
    "\n",
    "create external table Stackoverflow_tags  (id int, tag string) ;\n",
    "insert into table stackoverflow_tags SELECT Id, tag FROM stackoverflow_posts lateral view explode(split(tags,'\\;')) tags AS tag;\n",
    "\n",
    "\n",
    "create table joined_tag_date(id int,tag string ,creationdate string) row format delimited fields terminated by ',';\n",
    "insert into table joined_tag_date select t.id ,t.tag ,p.creationdate from stackoverflow_posts p inner join stackoverflow_tags t on p.id=t.id \n",
    "\n",
    "\n",
    "create table aggregate_tag_date(tag string ,creationdate string,count int) row format delimited fields terminated by ',';\n",
    "insert into table aggregate_tag_date select tag,creationdate,count(*) from joined_tag_date WHERE tag IS NOT NULL AND creationdate IS NOT NULL group by tag,creationdate;\n",
    "insert overwrite local directory '/bigtemp/DealingF18GB/aggregate_tag_date.csv' row format delimited fields terminated by ',' select * from aggregate_tag_date;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><strong>Note :</strong> The above code was run in Bigdata server and is not expected to run successfully in Notebooks.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Sentiment-Analysis-Using-NLTK-on-comments:\">Sentiment Analysis Using NLTK on comments:<a class=\"anchor-link\" href=\"#Sentiment-Analysis-Using-NLTK-on-comments:\">¶</a></h1><p>We calculated individual sentiment of every comment in comments.csv file contaings millions of rows.</p>\n",
    "<h2 id=\"The-below-code-was-run-on-our-local-machines.-The-comments.csv-file-was-downloaded-from-the-Bigdata-server-and-a-new-CSV-file-containing-the-sentiment-of-the-comments-were-generated-using-the-below-code.\">The below code was run on our local machines. The comments.csv file was downloaded from the Bigdata server and a new CSV file containing the sentiment of the comments were generated using the below code.<a class=\"anchor-link\" href=\"#The-below-code-was-run-on-our-local-machines.-The-comments.csv-file-was-downloaded-from-the-Bigdata-server-and-a-new-CSV-file-containing-the-sentiment-of-the-comments-were-generated-using-the-below-code.\">¶</a></h2><p>The code is as below:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python \n",
    "\n",
    "#import nltk\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import textblob\n",
    "#from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from textblob import TextBlob \n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "for line in sys.stdin:\n",
    "\n",
    "\tif(re.search(r'^$',line)):\n",
    "\t\tcontinue\n",
    "\telse:\n",
    "\t\tline = line.decode('utf-8')\n",
    "\t\tline = line.split(',')\n",
    "\t\tif(len(line)>3):\n",
    "\t\t\tif(RepresentsInt(line[0]) and RepresentsInt(line[1])):\n",
    "\t\t\t\tanalysis = TextBlob(line[3])\n",
    "\n",
    "\t\t\t\t# set sentiment \n",
    "\t\t\t\tif analysis.sentiment.polarity > 0: \n",
    "\t\t\t\t\tprint ('{},{},{}\\n').format(line[0].encode('utf-8'),line[1].encode('utf-8'),1),\n",
    "\t\t\t\telif analysis.sentiment.polarity == 0: \n",
    "\t\t\t\t\tprint ('{},{},{}\\n').format(line[0].encode('utf-8'),line[1].encode('utf-8'),0),\n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tprint ('{},{},{}\\n').format(line[0].encode('utf-8'),line[1].encode('utf-8'),-1),\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><strong>Note : </strong>The above code was run in Bigdata server and is not expected to run successfully in Notebooks.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>As of now, all the data was loaded in Hive tables. For easy analysis of our data in Python, we first ran the aggregation jobs in Hive. The resulting CSV files were then loaded to SQL tables.</p>\n",
    "<p>There were lot of steps involved in actually migrating the data from Hive to SQL. We followed the basic steps as :</p>\n",
    "<h2 id=\"Load-Hive-Data-into-a-CSV-on-HDFS-&gt;&gt;-Copy-the-CSV-file-from-HDFS-to-Local-&gt;&gt;-Upload-the-CSV-file-into-SQL-tables.\">Load Hive Data into a CSV on HDFS &gt;&gt; Copy the CSV file from HDFS to Local &gt;&gt; Upload the CSV file into SQL tables.<a class=\"anchor-link\" href=\"#Load-Hive-Data-into-a-CSV-on-HDFS-&gt;&gt;-Copy-the-CSV-file-from-HDFS-to-Local-&gt;&gt;-Upload-the-CSV-file-into-SQL-tables.\">¶</a></h2><p>Time being a constraint, we applied the above steps. However, there could be a smarter way of doing it. Which we plan on exploring in future projects.</p>\n",
    "<p>The below SQL tables were generated :</p>\n",
    "<ol>\n",
    "<li>Project_Tag_Date : This table stores the aggregate count of the posts data, which was grouped by Tags and Date fields. The resultant table has the below structure : </li>\n",
    "</ol>\n",
    "<ul>\n",
    "<li>Tag (varchar)</li>\n",
    "<li>CreationDate (date)</li>\n",
    "<li>Count (int)</li>\n",
    "</ul>\n",
    "<ol>\n",
    "<li>Project_Tag_Location : This table stores the aggregate count of the posts data, which was grouped by Tags and Location fields. The resultant table has the below structure :</li>\n",
    "</ol>\n",
    "<ul>\n",
    "<li>Tag (varchar)</li>\n",
    "<li>Location (varchar)</li>\n",
    "<li>Count (int)</li>\n",
    "</ul>\n",
    "<ol>\n",
    "<li>Project_Tag_Sentiment : This table stores the aggregate sentiment of the posts data, which was grouped by the Tags field. The resultant table has the below structure :</li>\n",
    "</ol>\n",
    "<ul>\n",
    "<li>Tag (varchar)</li>\n",
    "<li>Sentiment Score (int)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Data-cleaning-COMPLETE!!-Now-for-the-FUN-stuff---Analysis-in-Python\">Data cleaning COMPLETE!! Now for the FUN stuff - Analysis in Python<a class=\"anchor-link\" href=\"#Data-cleaning-COMPLETE!!-Now-for-the-FUN-stuff---Analysis-in-Python\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Step-1:-Installation-of-the-pre-requisite-packages\">Step 1: Installation of the pre-requisite packages<a class=\"anchor-link\" href=\"#Step-1:-Installation-of-the-pre-requisite-packages\">¶</a></h2><p><strong>Now starts the code which we have run on our Python Notebooks. </strong></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!sudo pip3 install -q -U sql_magic\n",
    "\n",
    "!sudo apt-get update\n",
    "\n",
    "!sudo apt-get install python-mysqldb\n",
    "\n",
    "!sudo apt-get install python3-dev libmysqlclient-dev\n",
    "\n",
    "!sudo pip3 install mysqlclient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Step-2:-Preparation-of-the-SQL-engine-for-accessing-MySQL-server-data\">Step 2: Preparation of the SQL engine for accessing MySQL server data<a class=\"anchor-link\" href=\"#Step-2:-Preparation-of-the-SQL-engine-for-accessing-MySQL-server-data\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy import create_engine\n",
    "conn_string = 'mysql://{user}:{password}@{host}/?charset=utf8'.format(\n",
    "    host = 'bigdata.stern.nyu.edu', \n",
    "    user = 'DealingF18GB8',\n",
    "    password = 'DealingF18GB8!!',\n",
    "    encoding = 'utf-8')\n",
    "engine = create_engine(conn_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%reload_ext sql_magic\n",
    "%config SQL.conn_name = 'engine'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%read_sql\n",
    "use DealingF18GB8;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%read_sql\n",
    "show tables ;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%read_sql \n",
    "describe DealingF18GB8.Project_Tag_Location;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%read_sql \n",
    "describe DealingF18GB8.Project_Tag_Date;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Step-3:-Importing-Pandas\">Step 3: Importing Pandas<a class=\"anchor-link\" href=\"#Step-3:-Importing-Pandas\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Step-4:-Query-aggregate-data-from-the-table\">Step 4: Query aggregate data from the table<a class=\"anchor-link\" href=\"#Step-4:-Query-aggregate-data-from-the-table\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%read_sql\n",
    "show tables ;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Code-for-Geotagging-the-locations\">Code for Geotagging the locations<a class=\"anchor-link\" href=\"#Code-for-Geotagging-the-locations\">¶</a></h1><h1 id=\"Why-Geotagging-is-important-in-our-project?\">Why Geotagging is important in our project?<a class=\"anchor-link\" href=\"#Why-Geotagging-is-important-in-our-project?\">¶</a></h1><p>As of now our data is in the table Project_Tag_Location table. However, the values present in the location field for the respective tags are not in uniform format. For eg, one row contains \"New York\" and another row contains \"United States\". This is not good.</p>\n",
    "<p>Hence, as an important step, we did a geotagging of our data. We queried the aggregate data from \"Project_Tag_Location\" table and grouped by location. You may notice that the limit has been set to 2400. The reason for putting a limit in our results is that we found the first 2400 records based on frequency to be of relevance.</p>\n",
    "<h1 id=\"How-we-did-Geotagging?\">How we did Geotagging?<a class=\"anchor-link\" href=\"#How-we-did-Geotagging?\">¶</a></h1><p>We used Google Map APIs for Geotagging our data. The Map API is not free of cost, but we receive $ 300 credit in the free trial period. We made the best utilization of that. :-)</p>\n",
    "<h1 id=\"Next-step\">Next step<a class=\"anchor-link\" href=\"#Next-step\">¶</a></h1><p>1st Step : After we did the Geotagging, it was important that the data be immediately stored in a resultant table, in case we faced any real time issues. We went ahead and stored the resultant data in a new table called \"Project_Tag_FreqByCity\".</p>\n",
    "<p>2nd Step : Now we have data distrubuted by Cities. We further aggregate the data in the \"Project_Tag_FreqByCity\" to another table called \"Project_Tag_FreqByCountry\".</p>\n",
    "<h2 id=\"Phew-!!-Lot-of-work-but-we-finally-have-a-precise-table-with-the-relevant-aggregated-data.-Stay-tuned-for-more.\">Phew !! Lot of work but we finally have a precise table with the relevant aggregated data. Stay tuned for more.<a class=\"anchor-link\" href=\"#Phew-!!-Lot-of-work-but-we-finally-have-a-precise-table-with-the-relevant-aggregated-data.-Stay-tuned-for-more.\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%read_sql df_location\n",
    "select location,SUM(count) freq from Project_Tag_Location WHERE tag IS NOT NULL AND tag != '' AND location != '' GROUP BY location ORDER BY freq DESC LIMIT 2400;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "GOOGLE_MAPS_API_URL = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "\n",
    "\n",
    "address = 'Bangalore'\n",
    "params = {\n",
    "    #'key':'AIzaSyC2L937vXMH3QpJIPHTs1uuH1H3dLsuH', #removed key due to request limits\n",
    "    'address': address,\n",
    "    'sensor': 'false',\n",
    "    'lang': 'en'\n",
    "}\n",
    "\n",
    "# Do the request and get the response data\n",
    "req = requests.get(GOOGLE_MAPS_API_URL, params=params)\n",
    "res = req.json()\n",
    "res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "GOOGLE_MAPS_API_URL = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "\n",
    "df_location_only = pd.DataFrame()\n",
    "for index, row in df_location.iterrows():\n",
    "    try:\n",
    "      df_location.loc[index,'location'] = row['location'].replace('|','')\n",
    "      address = row['location']\n",
    "      params = {\n",
    "          #'key':'AIzaSyC2L937vXMH3QpJIPHTs1uuH1H3dLsuH',  #removed key due to request limits\n",
    "          'address': address,\n",
    "          'sensor': 'false',\n",
    "          'lang': 'en'\n",
    "      }\n",
    "\n",
    "      # Do the request and get the response data\n",
    "      req = requests.get(GOOGLE_MAPS_API_URL, params=params)\n",
    "      res = req.json()\n",
    "\n",
    "      results = res['results']\n",
    "      listaddresscomp = results[0]['address_components']\n",
    "      location = results[0]['geometry']['location']\n",
    "      lat = location['lat']\n",
    "      lng = location['lng']\n",
    "\n",
    "      df_location.loc[index,'lat'] = lat\n",
    "      df_location.loc[index,'lng'] = lng\n",
    "\n",
    "      for addcomp in listaddresscomp: \n",
    "        longname = addcomp['long_name']\n",
    "        shortname = addcomp['short_name']\n",
    "        typename = addcomp['types'][0]\n",
    "\n",
    "        if(typename == 'country'):\n",
    "          countryname = longname\n",
    "          df_location.loc[index,'countryname'] = countryname\n",
    "\n",
    "      df_location_only.loc[index,'rawloc'] = row['location']\n",
    "      df_location_only.loc[index,'lat'] = row['lat']\n",
    "      df_location_only.loc[index,'lng'] = row['lng']\n",
    "      df_location_only.loc[index,'countryname'] = row['countryname']\n",
    "      df_location_only.loc[index,'freq'] = row['freq']\n",
    "    except Exception as e: \n",
    "      print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_location_only.fillna(value=\"\",inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import MySQLdb as mdb\n",
    "\n",
    "con = mdb.connect(host = 'bigdata.stern.nyu.edu', \n",
    "                  user = 'DealingF18GB8',                  \n",
    "                  passwd = 'DealingF18GB8!!', \n",
    "                  charset='utf8', use_unicode=True);\n",
    "\n",
    "\n",
    "# Query to create a database\n",
    "db_name = 'DealingF18GB8'\n",
    "create_db_query = \"CREATE DATABASE IF NOT EXISTS {db} DEFAULT CHARACTER SET 'utf8'\".format(db=db_name)\n",
    "\n",
    "# Create a database\n",
    "cursor = con.cursor()\n",
    "cursor.execute(create_db_query)\n",
    "cursor.close()\n",
    "\n",
    "\n",
    "cursor = con.cursor()\n",
    "table_name = 'Project_Locations'\n",
    "# Create a table\n",
    "# Drop it if it exists\n",
    "create_drop_locations='''drop table if exists {db}.{table}'''.format(db=db_name,table=table_name)\n",
    "\n",
    "cursor.execute(create_drop_locations)\n",
    "# The {db} and {table} are placeholders for the parameters in the format(....) statement\n",
    "create_table_query = '''CREATE TABLE IF NOT EXISTS {db}.{table}(rawloc varchar(255),lat varchar(255),lng varchar(255),countryname varchar(255),freq int)'''.format(db=db_name, table=table_name)\n",
    "cursor.execute(create_table_query)\n",
    "cursor.close()\n",
    "\n",
    "\n",
    "query_template = '''INSERT INTO {db}.{table}(rawloc,lat,lng,countryname,freq) VALUES (%s, %s, %s, %s, %s)'''.format(db=db_name, table=table_name)\n",
    "cursor = con.cursor()\n",
    "\n",
    "rawloc = ''\n",
    "formattedadd  = '' \n",
    "lat  = '' \n",
    "lng  = ''\n",
    "countryname  = ''\n",
    "freq = ''\n",
    "\n",
    "for index, row in df_location_only.iterrows():\n",
    "    rawloc = row['rawloc']\n",
    "    lat  = row['lat'] \n",
    "    lng  = row['lng'] \n",
    "    countryname  = row['countryname']\n",
    "    freq = row['freq']\n",
    "\n",
    "    \n",
    "    if(rawloc == None):\n",
    "      rawloc = ''\n",
    "    if(lat == None):\n",
    "      lat = ''\n",
    "    if(lng == None):\n",
    "      lng = ''\n",
    "    if(countryname == None):\n",
    "      countryname =''\n",
    "    if(freq == None):\n",
    "      freq = 0\n",
    "    query_parameters = (rawloc, lat, lng, countryname,freq)\n",
    "    cursor.execute(query_template, query_parameters)\n",
    "\n",
    "con.commit()\n",
    "cursor.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Step-5:-Now-plotting-our-Geotagged-data-in-a-world-map-to-identify-Developers-from-Different-countries.\">Step 5: Now plotting our Geotagged data in a world map to identify Developers from Different countries.<a class=\"anchor-link\" href=\"#Step-5:-Now-plotting-our-Geotagged-data-in-a-world-map-to-identify-Developers-from-Different-countries.\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!!sudo pip3 install -U geopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import MySQLdb as mdb\n",
    "\n",
    "con = mdb.connect(host = 'bigdata.stern.nyu.edu', \n",
    "                  user = 'DealingF18GB8',                  \n",
    "                  passwd = 'DealingF18GB8!!', \n",
    "                  charset='utf8', use_unicode=True);\n",
    "\n",
    "\n",
    "# use a database\n",
    "db_name = 'DealingF18GB8'\n",
    "\n",
    "#cursor = con.cursor()\n",
    "table_name = 'Project_Tag_FreqByCity'\n",
    "\n",
    ";\n",
    "\n",
    "query_country_freq=''' SELECT SUM(freq) as fr,countryname FROM {db}.{table} WHERE countryname != '' AND freq != 0 GROUP BY countryname ORDER BY fr DESC '''.format(db=db_name,table=table_name)\n",
    "\n",
    "df_country_freq = pd.read_sql(query_country_freq, con)\n",
    "\n",
    "df_country_freq.head(20)\n",
    "\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"nameofapp\")\n",
    "\n",
    "for index,row in df_country_freq.iterrows():\n",
    "    location = geolocator.geocode(row['countryname'])\n",
    "    df_country_freq.loc[index,'lat'] = location.latitude\n",
    "    df_country_freq.loc[index,'lng'] = location.longitude\n",
    "df_country_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!sudo pip3 install -U folium\n",
    "!sudo pip3 install -U geopandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Final-Outcomes\">Final Outcomes<a class=\"anchor-link\" href=\"#Final-Outcomes\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id='1.-From-the-Tag-data-present-in-\"Project_Tag_Date\"-table,-we-find-the-most-commonly-used-languages,-databases-and-frameworks.-The-data-is-then-plotted-in-a-bar-chart-for-easier-understanding.'>1. From the Tag data present in \"Project_Tag_Date\" table, we find the most commonly used languages, databases and frameworks. The data is then plotted in a bar chart for easier understanding.<a class=\"anchor-link\" href='#1.-From-the-Tag-data-present-in-\"Project_Tag_Date\"-table,-we-find-the-most-commonly-used-languages,-databases-and-frameworks.-The-data-is-then-plotted-in-a-bar-chart-for-easier-understanding.'>¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import MySQLdb as mdb\n",
    "\n",
    "con = mdb.connect(host = 'bigdata.stern.nyu.edu', \n",
    "                  user = 'DealingF18GB8',                  \n",
    "                  passwd = 'DealingF18GB8!!', \n",
    "                  charset='utf8', use_unicode=True);\n",
    "\n",
    "\n",
    "# use a database\n",
    "db_name = 'DealingF18GB8'\n",
    "#use_db_query = \"use {db}\".format(db=db_name)\n",
    "#cursor = con.cursor()\n",
    "#cursor.execute(use_db_query)\n",
    "#cursor.close()\n",
    "\n",
    "\n",
    "#cursor = con.cursor()\n",
    "table_name = 'Project_Tag_Date'\n",
    "\n",
    "query_tag_freq='''SELECT tag Community,count Number_Of_Posts FROM {db}.{table} WHERE tag NOT LIKE '20%' '''.format(db=db_name,table=table_name)\n",
    "\n",
    "df_tag_freq = pd.read_sql(query_tag_freq, con)\n",
    "\n",
    "df_tag_freq.head(20)\n",
    "\n",
    "\n",
    "df_tag_by_freq = df_tag_freq.groupby('Community')['Number_Of_Posts'].sum().sort_values().tail(15)\n",
    "\n",
    "\n",
    "df_tag_by_freq.plot(kind='barh',figsize=(20,20),title=\"Top Developer Communities\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"2.-From-top-100-languages,-we-find-below-:\">2. From top 100 languages, we find below :<a class=\"anchor-link\" href=\"#2.-From-top-100-languages,-we-find-below-:\">¶</a></h2><p>A.   The most satisfied developer communities</p>\n",
    "<p>B.   The most frustrated developer communities</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"A.-The-most-satisfied-developer-communities\">A. The most satisfied developer communities<a class=\"anchor-link\" href=\"#A.-The-most-satisfied-developer-communities\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import MySQLdb as mdb\n",
    "\n",
    "con = mdb.connect(host = 'bigdata.stern.nyu.edu', \n",
    "                  user = 'DealingF18GB8',                  \n",
    "                  passwd = 'DealingF18GB8!!', \n",
    "                  charset='utf8', use_unicode=True);\n",
    "\n",
    "db_name = 'DealingF18GB8'\n",
    "table_name = 'Project_Tag_Date'\n",
    "query_tag_freq_ordered='''SELECT tag,SUM(count) freq FROM {db}.{table} WHERE tag != '' AND tag NOT LIKE '20%' GROUP BY tag ORDER BY freq DESC LIMIT 200'''.format(db=db_name,table=table_name)\n",
    "\n",
    "df_tag_freq_ordered = pd.read_sql(query_tag_freq_ordered, con)\n",
    "\n",
    "\n",
    "\n",
    "  #SELECT tag,SUM(sentiment) FROM DealingF18GB8.Project_Tag_Sentiment WHERE tag != '' AND tag NOT LIKE '20%' GROUP BY tag;\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  # use a database\n",
    "\n",
    "  #use_db_query = \"use {db}\".format(db=db_name)\n",
    "  #cursor = con.cursor()\n",
    "  #cursor.execute(use_db_query)\n",
    "  #cursor.close()\n",
    "\n",
    "query_tag_sentiment = {}\n",
    "df_tag_sentiment = {}\n",
    "  #cursor = con.cursor()\n",
    "for index,row in df_tag_freq_ordered.iterrows():\n",
    "    table_name = 'Project_Tag_Sentiment'\n",
    "    key = row[\"tag\"]\n",
    "    query_tag_sentiment ='''SELECT tag Community,sentiment totalsentiment FROM {db}.{table} WHERE tag != '' AND tag NOT LIKE '20%' AND tag = '{tagval}' '''.format(db=db_name,table=table_name,tagval=key)\n",
    "\n",
    "    df_tag_sentiment[key] = pd.read_sql(query_tag_sentiment, con)\n",
    "df_list = []\n",
    "\n",
    "df_communities = pd.concat(df_tag_sentiment.values())\n",
    "\n",
    "df_communities['Satisfaction(%)'] = 100*df_communities['totalsentiment']/sum(df_communities['totalsentiment'])\n",
    "\n",
    "\n",
    "\n",
    "df_happiest_communities = df_communities.sort_values('Satisfaction(%)',ascending=False)\n",
    "df_happiest_communities = df_happiest_communities.iloc[:15]\n",
    "ax = df_happiest_communities.plot.barh(x='Community', y='Satisfaction(%)',figsize=(20,20),title='Happiest Developer Communities')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"B.-The-most-frustrated-developer-communities\">B. The most frustrated developer communities<a class=\"anchor-link\" href=\"#B.-The-most-frustrated-developer-communities\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import MySQLdb as mdb\n",
    "\n",
    "con = mdb.connect(host = 'bigdata.stern.nyu.edu', \n",
    "                  user = 'DealingF18GB8',                  \n",
    "                  passwd = 'DealingF18GB8!!', \n",
    "                  charset='utf8', use_unicode=True);\n",
    "\n",
    "db_name = 'DealingF18GB8'\n",
    "table_name = 'Project_Tag_Date'\n",
    "query_tag_freq_ordered='''SELECT tag,SUM(count) freq FROM {db}.{table} WHERE tag != '' AND tag NOT LIKE '20%' GROUP BY tag ORDER BY freq DESC LIMIT 200'''.format(db=db_name,table=table_name)\n",
    "\n",
    "df_tag_freq_ordered = pd.read_sql(query_tag_freq_ordered, con)\n",
    "\n",
    "\n",
    "\n",
    "  #SELECT tag,SUM(sentiment) FROM DealingF18GB8.Project_Tag_Sentiment WHERE tag != '' AND tag NOT LIKE '20%' GROUP BY tag;\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "  # use a database\n",
    "\n",
    "  #use_db_query = \"use {db}\".format(db=db_name)\n",
    "  #cursor = con.cursor()\n",
    "  #cursor.execute(use_db_query)\n",
    "  #cursor.close()\n",
    "\n",
    "query_tag_sentiment = {}\n",
    "df_tag_sentiment = {}\n",
    "  #cursor = con.cursor()\n",
    "for index,row in df_tag_freq_ordered.iterrows():\n",
    "    table_name = 'Project_Tag_Sentiment'\n",
    "    key = row[\"tag\"]\n",
    "    query_tag_sentiment ='''SELECT tag Community,sentiment totalsentiment FROM {db}.{table} WHERE tag != '' AND tag NOT LIKE '20%' AND tag = '{tagval}' '''.format(db=db_name,table=table_name,tagval=key)\n",
    "\n",
    "    df_tag_sentiment[key] = pd.read_sql(query_tag_sentiment, con)\n",
    "df_list = []\n",
    "\n",
    "df_communities = pd.concat(df_tag_sentiment.values())\n",
    "\n",
    "df_communities['Satisfaction(%)'] = 100*df_communities['totalsentiment']/sum(df_communities['totalsentiment'])\n",
    "\n",
    "\n",
    "df_frustrated_communities = df_communities.sort_values('Satisfaction(%)',ascending=True)\n",
    "df_frustrated_communities = df_frustrated_communities.iloc[:15]\n",
    "ax = df_frustrated_communities.plot.barh(x='Community', y='Satisfaction(%)',figsize=(20,20),title='Frustrated Developer Communities')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"3.-Here's-the-plot-on-the-world-map.-Check-the-countries-with-highest-number-of-developers\">3. Here's the plot on the world map. Check the countries with highest number of developers<a class=\"anchor-link\" href=\"#3.-Here's-the-plot-on-the-world-map.-Check-the-countries-with-highest-number-of-developers\">¶</a></h1><p>This is an interactive map provided by folium. We can zoom in and click on the circles to see the name of the countries.</p>\n",
    "<p>As of today, we can see that the countries with the highest number of developers are in USA, India and UK.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import folium\n",
    "fmap = folium.Map(location=[20,0], zoom_start=2, tiles=\"Mapbox Bright\")\n",
    "fmap\n",
    "\n",
    "totalfreq = sum(df_country_freq[\"fr\"])\n",
    "for name, row in df_country_freq.iterrows():\n",
    "    # Define the opacity of the marker to be proportional to the percentage of bikes in the station\n",
    "    opacity = 0.5\n",
    "    # Make the color green for the working stations, red otherwise\n",
    "    color = \"cyan\"\n",
    "    # The size of the marker is proportional to the number of docks\n",
    "    size = row[\"fr\"]/500000\n",
    "\n",
    "    # We create a marker on the map and we add it to the map\n",
    "    folium.CircleMarker(location=[row[\"lat\"], row[\"lng\"]],radius = size,fill=True,fill_opacity = opacity,color=color,fill_color = color,popup=row[\"countryname\"]+\"(\"+ str(int(row[\"fr\"]))+\" posts)\").add_to(fmap)\n",
    "fmap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Conclusions-:\">Conclusions :<a class=\"anchor-link\" href=\"#Conclusions-:\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We observed that most popular programming language is javascript and Java, whereas most satisfied developer communities are javascript and java programmer. \n",
    "Hence the might conclude that since javscript and java developer are most satisfied,  causing the language to be more popular.\n",
    "Beside the most frustrated developer community is elasticeserch and reactjs users.</p>\n",
    "<p>Also maximum contributions are from USA,India,UK and Germany region.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Lessons-Learned-:\">Lessons Learned :<a class=\"anchor-link\" href=\"#Lessons-Learned-:\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>We understood the difficulties faced during data cleaning phase, mostly time required to process the huge amount of data. The Hadoop framework helps in substantially decreasing these time in real-world. Further, even after data cleaning, we faced discrepancies in a data file and we had to ignore those entries in our analysis. We did write python program for data cleaning, which is fast, efficient and easy to write. We did sentiment analysis on comments to figure out overall satisfaction of different programmer communities. After aggregating data on Hive QL, we imported it into SQL tables and data frames. This helped in understanding the whole process of getting a piece of valuable information from raw data and using it to get better insights on correlations between different unrelated variables after the visualization step.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Thank-you\">Thank you<a class=\"anchor-link\" href=\"#Thank-you\">¶</a></h1>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
